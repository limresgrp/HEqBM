# general
root: results/PED
run_name: tutorial
seed: 0
dataset_seed: 0
append: true
default_dtype: float32
device: cuda:0

# -- network --
model_builders:
  - heqbm.backmapping.model.HierarchicalBackmapping
  - heqbm.backmapping.model.HierarchicalReconstruction

# cutoffs
r_max: 7.0
avg_num_neighbors: auto

# radial basis
BesselBasis_trainable: false
PolynomialCutoff_p: 18
num_basis: 8

# symmetry
l_max: 3
parity: o3_full

# layers:
num_layers: 2
env_embed_multiplicity: 128
embed_initial_edge: true

inv_out_irreps: 2x0e    # Predict Phi and Psi, which are invariants
eq_out_irreps:  5x1o    # Max number of atoms per bead is 5, we predict up to 5 distance vectors
readout_pre_pooling: true
normalize_out_features: true

env_embed_mlp_latent_dimensions: [512]
env_embed_mlp_nonlinearity: null
env_embed_mlp_initialization: uniform
# env_embed_exp_function_normalization_constant: 1.

two_body_latent_mlp_latent_dimensions: [128, 256, 512, 512]
two_body_latent_mlp_nonlinearity: silu
two_body_latent_mlp_initialization: uniform
# two_body_latent_exp_function_normalization_constant: 1.

latent_mlp_latent_dimensions: [512, 512]
latent_mlp_nonlinearity: silu
latent_mlp_initialization: uniform
latent_resnet: true
# latent_exp_function_normalization_constant: .5

product_correlation: 3   # Maximum correlation order of tensor product expansion

# - end layers -

# Final MLP to go from latent space to edge energies:
edge_eng_mlp_latent_dimensions: [128]
edge_eng_mlp_nonlinearity: null
edge_eng_mlp_initialization: uniform

normalize_edge_energy_sum: false

# -- data --
directed_graph: true
dataset_list:
  - dataset: folder_npz
    dataset_file_name: --- /YOUR/PATH/TO/NPZ/TRAINING/DATASET(S)/FOLDER --- # /storage_common/angiod/PED/backmapping/npz/train/
    key_mapping:
      bead_types: bead_numbers
      bead_pos: pos
      bead2atom_rel_vectors: eq_atom_features
      bb_phipsi: inv_atom_features                # Include for BB optimization
      ca_next_direction: eq_atom_input_features   # Include for BB optimization
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
    npz_fixed_field_keys:
    - bead_numbers
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_bond_idx
    - atom_angle_idx
    include_keys:
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_pos

validation_dataset_list:
  - validation_dataset: folder_npz
    validation_dataset_file_name: --- /YOUR/PATH/TO/NPZ/VALIDATION/DATASET(S)/FOLDER --- # /storage_common/angiod/PED/backmapping/npz/valid/
    key_mapping:
      bead_types: bead_numbers
      bead_pos: pos
      bead2atom_rel_vectors: eq_atom_features
      bb_phipsi: inv_atom_features                # Include for BB optimization
      ca_next_direction: eq_atom_input_features   # Include for BB optimization
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
    npz_fixed_field_keys:
    - bead_numbers
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_bond_idx
    - atom_angle_idx
    include_keys:
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_pos

type_names:
- ACE_RE
- ALA_BB
- ALA_SC1
- ARG_BB
- ARG_SC1
- ARG_SC2
- ASN_BB
- ASN_SC1
- ASP_BB
- ASP_SC1
- CYS_BB
- CYS_SC1
- CYX_BB
- CYX_SC1
- GLN_BB
- GLN_SC1
- GLU_BB
- GLU_SC1
- GLY_BB
- HID_BB
- HID_SC1
- HID_SC2
- HID_SC3
- HIE_BB
- HIE_SC1
- HIE_SC2
- HIE_SC3
- HIS_BB
- HIS_SC1
- HIS_SC2
- HIS_SC3
- ILE_BB
- ILE_SC1
- LEU_BB
- LEU_SC1
- LYS_BB
- LYS_SC1
- LYS_SC2
- MET_BB
- MET_SC1
- NME_RE
- PHE_BB
- PHE_SC1
- PHE_SC2
- PHE_SC3
- PRO_BB
- PRO_SC1
- SER_BB
- SER_SC1
- THR_BB
- THR_SC1
- TRP_BB
- TRP_SC1
- TRP_SC2
- TRP_SC3
- TRP_SC4
- TRP_SC5
- TYR_BB
- TYR_SC1
- TYR_SC2
- TYR_SC4
- TYR_SC3
- VAL_BB
- VAL_SC1

# logging
wandb: false
wandb_project: heqbm-tutorial-PED
verbose: info
log_batch_freq: 100

# training
batch_size: 1
validation_batch_size: 1

# Configure maximum batch sizes to avoid GPU memory errors. This parameters have to be configured according to your GPU RAM #

batch_max_edges: 50000              # Limit the maximum number of edges of a graph to be loaded on memory in a single batch
batch_max_atoms: 1000               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

#############################################################################################################################

max_epochs: 100000
learning_rate: 1.0e-3
train_val_split: random
shuffle: true
metrics_key: validation_loss

# use an exponential moving average of the weights
use_ema: true
ema_decay: 0.99
ema_use_num_updates: true

# loss function
loss_coeffs:
  - inv_atom_features:     # Loss on Phi and Psi dihedral prediction, used for Backbone optimization
    - 10.
    - DihMSELoss
    - TrainOnDelta: false
      ignore_nan: true
      ignore_zeroes: true
  - atom_pos:              # Loss on reconstructed atoms RMSD
    - 1.
    - MSELoss
    - ignore_pred_nan: true
  - atom_pos:              # Loss on bond and angle values of reconstructed atoms
    - 5.
    - InvMSELoss
    - TrainOnDelta: false
      ignore_nan: true
      ignore_zeroes: true

# optimizer
optimizer_name: Adam
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.

metrics_components:
  - - inv_atom_features
    - mae
    - ignore_nan: True
  - - atom_pos
    - mae
    - ignore_nan: True
  - - eq_atom_features
    - mae
    - ignore_nan: True

# lr scheduler, drop lr if no improvement for tot epochs
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 10
lr_scheduler_factor: 0.5

early_stopping_lower_bounds:
  LR: 1.0e-6

early_stopping_patiences:
  validation_loss: 10

per_edge_species_scale: 1.