# general
root: results/ligands
run_name: zma
seed: 0
dataset_seed: 0
append: true
default_dtype: float32
device: cuda

# -- network --
model_builders:
  - heqbm.backmapping.model.HierarchicalBackmapping
  - heqbm.backmapping.model.HierarchicalReconstruction

# cutoffs
r_max: 7.0
avg_num_neighbors: auto

# radial basis
BesselBasis_trainable: false
PolynomialCutoff_p: 18
num_basis: 8

# symmetry
l_max: 3
parity: o3_full

# layers:
num_layers: 2
env_embed_multiplicity: 128
embed_initial_edge: true

inv_out_irreps: "0x0e"    # Do NOT predict Phi and Psi, which are invariants, they are used only for BB optimization
eq_out_irreps: 4x1o
readout_pre_pooling: true
normalize_out_features: true

env_embed_mlp_latent_dimensions: [512]
env_embed_mlp_nonlinearity: null
env_embed_mlp_initialization: uniform
# env_embed_exp_function_normalization_constant: 1.

two_body_latent_mlp_latent_dimensions: [128, 256, 512, 512]
two_body_latent_mlp_nonlinearity: silu
two_body_latent_mlp_initialization: uniform
# two_body_latent_exp_function_normalization_constant: 1.

latent_mlp_latent_dimensions: [512, 512]
latent_mlp_nonlinearity: silu
latent_mlp_initialization: uniform
latent_resnet: true
# latent_exp_function_normalization_constant: .5

product_correlation: 3   # Maximum correlation order of tensor product expansion

# - end layers -

# Final MLP to go from latent space to edge energies:
edge_eng_mlp_latent_dimensions: [128]
edge_eng_mlp_nonlinearity: null
edge_eng_mlp_initialization: uniform

normalize_edge_energy_sum: false

# -- data --
directed_graph: true
dataset_list:
  - dataset: folder_npz
    dataset_file_name: --- /YOUR/PATH/TO/NPZ/TRAIN/DATASET(S)/FOLDER ---
    key_mapping:
      bead_types: bead_numbers
      bead_pos: pos
      bead2atom_rel_vectors: eq_atom_features
      bond_idcs: atom_bond_idcs
      angle_idcs: atom_angle_idcs
    npz_fixed_field_keys:
    - bead_numbers
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_bond_idcs
    - atom_angle_idcs
    include_keys:
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_pos

type_names:
- ZMA_B8
- ZMA_B1
- ZMA_B3
- ZMA_B10
- ZMA_B11
- ZMA_B9
- ZMA_B2
- ZMA_B6
- ZMA_B5
- ZMA_B7
- ZMA_B4

# logging
wandb: false
wandb_project: heqbm-ligands-zma
verbose: info
log_batch_freq: 100

# training
n_train: [950]
n_val: [50]
batch_size: 1
validation_batch_size: 1

# Configure maximum batch sizes to avoid GPU memory errors. This parameters have to be configured according to your GPU RAM #

batch_max_edges: 100000             # Limit the maximum number of edges of a graph to be loaded on memory in a single batch
batch_max_atoms: 1500               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

#############################################################################################################################

max_epochs: 100000
learning_rate: 1.0e-3
train_val_split: random
shuffle: true
metrics_key: validation_loss

# use an exponential moving average of the weights
use_ema: true
ema_decay: 0.99
ema_use_num_updates: true

# loss function
loss_coeffs:
  - atom_pos: 1.           # Loss on reconstructed atoms RMSD
  - atom_pos:              # Loss on bond and angle values of reconstructed atoms
    - 5.
    - InvMSELoss
    - TrainOnDelta: false
      ignore_nan: true
      ignore_zeroes: true

# optimizer
optimizer_name: Adam
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.

metrics_components:
  - - eq_atom_features
    - mae

# lr scheduler, drop lr if no improvement for tot epochs
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 3
lr_scheduler_factor: 0.5

early_stopping_lower_bounds:
  LR: 1.0e-6

early_stopping_patiences:
  validation_loss: 3

per_edge_species_scale: 1.