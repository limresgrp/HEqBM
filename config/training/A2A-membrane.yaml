# general
root: results/A2A-membrane
run_name: full-membrane
seed: 0
dataset_seed: 0
append: true
default_dtype: float32
device: cuda:3

# -- network --
model_builders:
  - heqbm.backmapping.model.HierarchicalBackmapping
  - heqbm.backmapping.model.HierarchicalReconstruction

# cutoffs
r_max: 20.0
avg_num_neighbors: auto

# radial basis
BesselBasis_trainable: false
PolynomialCutoff_p: 18
num_basis: 8

# symmetry
l_max: 2
parity: o3_full

# layers:
num_layers: 2
env_embed_multiplicity: 64
embed_initial_edge: true

inv_out_irreps: "0x0e"
eq_out_irreps: 6x1o
readout_pre_pooling: true
normalize_out_features: true

env_embed_mlp_latent_dimensions: [256]
env_embed_mlp_nonlinearity: null
env_embed_mlp_initialization: uniform
# env_embed_exp_function_normalization_constant: 1.

two_body_latent_mlp_latent_dimensions: [64, 128, 256, 256]
two_body_latent_mlp_nonlinearity: silu
two_body_latent_mlp_initialization: uniform
# two_body_latent_exp_function_normalization_constant: 1.

latent_mlp_latent_dimensions: [256, 256]
latent_mlp_nonlinearity: silu
latent_mlp_initialization: uniform
latent_resnet: true
# latent_exp_function_normalization_constant: .5

product_correlation: 2   # Maximum correlation order of tensor product expansion

# - end layers -

# Final MLP to go from latent space to edge energies:
edge_eng_mlp_latent_dimensions: [128]
edge_eng_mlp_nonlinearity: null
edge_eng_mlp_initialization: uniform

normalize_edge_energy_sum: false

# -- data --
directed_graph: true
dataset_list:
  - dataset: folder_npz
    dataset_file_name: /storage_common/angiod/A2A/backmapping/npz/membrane/train/
    key_mapping:
      bead_types: bead_numbers
      bead_pos: pos
      bead2atom_rel_vectors: eq_atom_features
      bond_idcs: atom_bond_idx
      angle_idcs: atom_angle_idx
    npz_fixed_field_keys:
    - bead_numbers
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_bond_idx
    - atom_angle_idx
    include_keys:
    - bead2atom_idcs
    - lvl_idcs_mask
    - lvl_idcs_anchor_mask
    - atom_pos

type_names:
- CHL_R1
- CHL_ROH
- CHL_R2
- CHL_R3
- CHL_R4
- CHL_C1
- CHL_R6
- CHL_R5
- CHL_C2
- CHL_C3
- OL_C1A
- OL_D2A
- OL_C3A
- OL_C4A
- PA_C1B
- PA_C2B
- PA_C3B
- PA_C4B
- PC_NC3
- PC_PO4
- PC_GL1
- PC_GL2

# logging
wandb: false
wandb_project: heqbm-a2a-membrane-full
verbose: info
log_batch_freq: 1

# training
n_train: [10]
n_val: [5]
batch_size: 1
validation_batch_size: 1

# Configure maximum batch sizes to avoid GPU memory errors. This parameters have to be configured according to your GPU RAM and n #

batch_max_edges: 40000              # Limit the maximum number of edges of a graph to be loaded on memory in a single batch
batch_max_atoms: 1000               # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

#############################################################################################################################

max_epochs: 100000
learning_rate: 1.0e-3
train_val_split: random
shuffle: true
metrics_key: validation_loss

# use an exponential moving average of the weights
use_ema: true
ema_decay: 0.99
ema_use_num_updates: true

# loss function
loss_coeffs:
  - atom_pos:              # Loss on reconstructed atoms RMSD
    - 1.
    - MSELoss
    - ignore_nan: true
      ignore_pred_nan: true
  - atom_pos:              # Loss on bond and angle values of reconstructed atoms
    - 5.
    - InvMSELoss
    - TrainOnDelta: false
      ignore_nan: true
      ignore_zeroes: true

# optimizer
optimizer_name: Adam
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.

metrics_components:
  - - atom_pos
    - mae
    - ignore_nan: True
  - - eq_atom_features
    - mae
    - ignore_nan: True

# lr scheduler, drop lr if no improvement for tot epochs
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_patience: 3
lr_scheduler_factor: 0.5

early_stopping_lower_bounds:
  LR: 1.0e-6

early_stopping_patiences:
  validation_loss: 3

per_edge_species_scale: 1.